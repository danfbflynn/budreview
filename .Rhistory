rm(list=ls())
options(stringsAsFactors=FALSE)
library(ncdf4)
library(dplyr)
library(Interpol.T)
library(chillR)
setwd("~/git/ospree")
d3 <- read.csv("analyses/output/ospree_clean.csv")#
#this file should use the cleaned data file created from Lizzie's "cleanmerge_all.R" code
colnames(d3)[17]<-"fsdate_tofix"#the date format in this new file needs to be changed, for this code to work
d3$fieldsample.date<-strptime(strptime(d3$fsdate_tofix, format = "%d-%b-%Y"),format = "%Y-%m-%d")
d <- subset(d3, woody=="yes")
# make two data frames. North America and Europe, the lat longs and years.
d$continent <- tolower(d$continent)
d$datasetID <- as.character(d$datasetID)
d$provenance.lat <- as.numeric(as.character(d$provenance.lat))
d$provenance.long <- as.numeric(as.character(d$provenance.long))
d$year <- as.numeric(as.character(d$year))
d$fieldsample.date<-as.character(as.Date(d$fieldsample.date,"%m/%d/%y")) #needed for new version
d <- as_data_frame(d)
##add new column that combines datasetID, provenance.lat, provenance.long, and field sample.date for later indexing
d$ID_fieldsample.date<-paste(d$datasetID,d$provenance.lat,d$provenance.long,d$fieldsample.date, sep="_")
# European studies
#want table with lat, long, year, field sample date for each study. there could  be multiple field sample dates for each study
#selecting out european studies using the dplyr package
eur <- d %>% # start with the data frame
distinct(ID_fieldsample.date, .keep_all = TRUE) %>% # establishing grouping variables
filter(continent == 'europe' & year >= 1950) %>%#select out europe
dplyr::select(datasetID, provenance.lat, provenance.long, year,fieldsample.date, ID_fieldsample.date)
eur <- eur[apply(eur, 1, function(x) all(!is.na(x))),] # only keep rows of all not na
# North America studies
nam <- d %>% # start with the data frame
distinct(ID_fieldsample.date, .keep_all = TRUE) %>% # establishing grouping variables
filter(continent == 'north america'& year >= 1950) %>%
dplyr::select(datasetID, provenance.lat, provenance.long, year,fieldsample.date, ID_fieldsample.date)
nam <- nam[apply(nam, 1, function(x) all(!is.na(x))),] # only keep rows of all not na
# these will need manual cleaning. for now use as is
duplicated(nam$ID_fieldsample.date); duplicated(eur$ID_fieldsample.date)
climatedrive = "/Volumes/Ospree Climate" # "/Volumes/Ospree Climate (Ospree Climate is name of new external drive, change with new device)
eur.tempmn <- nc_open(file.path(climatedrive, "tn_0.25deg_reg_v12.0.nc"))
eur.tempmx <- nc_open(file.path(climatedrive, "tx_0.25deg_reg_v12.0.nc"))
head(tempval)
tempval <- list()
for(i in 1:nrow(eur)){ # i = 1
# find this location
lo <- eur[i,"provenance.long"]
la <- eur[i,"provenance.lat"]
ndiff.long.cell <- abs(eur.tempmn$dim$longitude$vals-as.numeric(lo))
ndiff.lat.cell <- abs(eur.tempmn$dim$latitude$vals-as.numeric(la))
nlong.cell <- which(ndiff.long.cell==min(ndiff.long.cell))[1]
nlat.cell <- which(ndiff.lat.cell==min(ndiff.lat.cell))[1]
xdiff.long.cell <- abs(eur.tempmx$dim$longitude$vals-as.numeric(lo))
xdiff.lat.cell <- abs(eur.tempmx$dim$latitude$vals-as.numeric(la))
xlong.cell <- which(xdiff.long.cell==min(xdiff.long.cell))[1]
xlat.cell <- which(xdiff.lat.cell==min(xdiff.lat.cell))[1]
yr <- as.numeric(eur[i,"year"])#need to add month
# start and end days, in days since baseline date. Set to GMT to avoid daylight savings insanity
stday <- strptime(paste(yr-1, "09-01", sep="-"),"%Y-%m-%d", tz="GMT")
if(eur[i,"fieldsample.date"]!="" & as.numeric(substr(eur[i,"fieldsample.date"],6,7))>=9){stday <- strptime(paste(yr, "09-01", sep="-"),"%Y-%m-%d", tz="GMT")}
if(eur[i,"fieldsample.date"]!="" & as.numeric(substr(eur[i,"fieldsample.date"],6,7))<9){stday <- strptime(paste(yr-1, "09-01", sep="-"),"%Y-%m-%d", tz="GMT")}
if(eur[i,"fieldsample.date"]==""){stday <- strptime(paste(yr-1, "09-01", sep="-"),"%Y-%m-%d", tz="GMT")}
# using d$fieldsample.date
if(eur[i,"fieldsample.date"]!=""){endday <- strptime(eur[i,"fieldsample.date"],"%Y-%m-%d", tz = "GMT")}
if(eur[i,"fieldsample.date"]==""){endday <- strptime(paste(yr, "04-30", sep="-"),"%Y-%m-%d", tz = "GMT")}
st <- as.numeric(as.character(stday - strptime("1950-01-01", "%Y-%m-%d", tz = "GMT")))
en <- as.numeric(as.character(endday - strptime("1950-01-01", "%Y-%m-%d", tz = "GMT")))
if(en<st){en=st}
if(endday<stday){endday=stday}
# get temperature values for this range.
# check the dim of the net cdf file, str(netcdf), and see what the order of the different dimensions are. In this case, it goes long, lat, time. So when we are moving through the file, we give it the long and lat and date of start, then move through the files by going 'up' the cube of data to the end date
mins <- ncvar_get(eur.tempmn,'tn',
start=c(nlong.cell,nlat.cell,st),
count=c(1,1,en-st+1) # this is where we move through the 'cube' to get the one vector of Temp mins
)
maxs <- ncvar_get(eur.tempmx,'tx',
start=c(xlong.cell,xlat.cell,st),
count=c(1,1,en-st+1)
)
tempval[[as.character(eur[i,"ID_fieldsample.date"])]] <- data.frame(Lat = la,Long = lo,Date = seq(stday, endday, by = "day"),
Tmin = mins, Tmax = maxs)#
}
head(tempval)
names(tempval)
nc_close(eur.tempmx)
nc_close(eur.tempmn)
######################################################
# North America
######################################################
nafiles <- dir(climatedrive)[grep("livneh", dir(climatedrive))]
for(i in 1:nrow(nam)){ # i = 1
# find this location
lo <- nam[i,"provenance.long"]
la <- nam[i,"provenance.lat"]
# make sure longitudes are negative, need to be for North America
if(lo > 0) { lo = lo*-1 }
yr <- as.numeric(nam[i,"year"])
# using d$fieldsample.date
if(nam[i,"fieldsample.date"]!=""){endday <- strptime(nam[i,"fieldsample.date"],"%Y-%m-%d", tz = "GMT")}
if(nam[i,"fieldsample.date"]==""){endday <- strptime(paste(yr, "04-30", sep="-"),"%Y-%m-%d", tz = "GMT")} #if no sampling date given, use april 30
if(substr(endday,1,4)==yr & as.numeric(substr(endday,6,7))<=9){#when sampling occurred in same year as study and when collection occurred before that year's sept chilling,
stday <- strptime(paste(yr-1, "09-01", sep="-"),"%Y-%m-%d", tz="GMT")
prevmo <- paste(yr-1, formatC(9:12, width=2, flag="0"), sep="");# use previous year's fall months of chilling (Sept-Dec)
endmo<-substr(endday,6,7);#month of sampling date
thismo <- paste(yr, formatC(1:endmo, width=2, flag="0"), sep="")#months from current year of chilling, through sampling date (Jan-whenever sampled)
chillmo<-c(prevmo, thismo)
}
if(substr(endday,1,4)==yr-1 & as.numeric(substr(endday,6,7))<=12  & as.numeric(substr(endday,6,7))>=9){#when sampling occurred in previous year as study only
stday <- strptime(paste(yr-1, "09-01", sep="-"),"%Y-%m-%d", tz="GMT")
prevmo <- paste(yr-1, formatC(9:substr(endday,6,7), width=2, flag="0"), sep="");# use previous year's fall months of chilling (Sept-whenever collection occured)}
chillmo<-prevmo
}
if(substr(endday,1,4)==yr & as.numeric(substr(endday,6,7))>=9){#when sampling occurred in same year as study and after chilling started that year
stday <- strptime(paste(yr, "09-01", sep="-"),"%Y-%m-%d", tz="GMT")
prevmo <- paste(yr, formatC(9:substr(endday,6,7), width=2, flag="0"), sep="");# use previous year's fall months of chilling (Sept-whenever collection occured)}
chillmo<-prevmo
}
if(substr(endday,1,4)==yr-1 & as.numeric(substr(endday,6,7))<=12  & as.numeric(substr(endday,6,7))>=9){#when sampling occurred in previous year as study between sept and dec
stday <- strptime(paste(yr-1, "09-01", sep="-"),"%Y-%m-%d", tz="GMT")
prevmo <- paste(yr-1, formatC(9:substr(endday,6,7), width=2, flag="0"), sep="");# use previous year's fall months of chilling (Sept-whenever collection occured)}
chillmo<-prevmo
}
if(substr(endday,1,4)==yr-1 & as.numeric(substr(endday,6,7))<=12  & as.numeric(substr(endday,6,7))<9){#when sampling occurred in previous year as study, NOT during the fall
stday <- strptime(paste(as.numeric(substr(endday,1,4))-1, "09-01", sep="-"),"%Y-%m-%d", tz="GMT")
prevmo <- paste(as.numeric(substr(endday,1,4))-1, formatC(9:12, width=2, flag="0"), sep="");# use previous year's fall months of chilling (Sept-Dec)
endmo<-substr(endday,6,7);#month of sampling date
thismo <- paste(as.numeric(substr(endday,1,4)), formatC(1:endmo, width=2, flag="0"), sep="")#months from current year of chilling, through sampling date (Jan-whenever sampled)
chillmo<-c(prevmo, thismo)
}
# now loop over these year-month combo files
mins <- maxs <- vector()
for(j in c(chillmo)){ # j = "200009"
file <- file.path(climatedrive,nafiles[grep(j, nafiles)])
jx <- nc_open(file)
diff.long.cell <- abs(jx$dim$lon$vals-as.numeric(lo))
diff.lat.cell <- abs(jx$dim$lat$vals-as.numeric(la))
long.cell <- which(diff.long.cell==min(diff.long.cell))[1]
lat.cell <- which(diff.lat.cell==min(diff.lat.cell))[1]
mins <- c(mins, ncvar_get(jx,'Tmin',start=c(long.cell,lat.cell,1),count=c(1,1,-1)))
maxs <- c(maxs, ncvar_get(jx,'Tmax',start=c(long.cell,lat.cell,1),count=c(1,1,-1)))
}
tempval[[as.character(nam[i,"ID_fieldsample.date"])]] <- data.frame(Lat = la,Long = lo,Date = as.character(seq(stday, endday, by = "day")),
Tmin = mins[1:length(seq(stday, endday, by = "day"))], Tmax =maxs[1:length(seq(stday, endday, by = "day"))])#
}
tail(tempval)
calibration_l = list(
Average = data.frame(time_min = rep(5, 12),
time_max = rep(14, 12),
time_suns = rep(17, 12),
C_m = rep(0.35, 12))
)
chillcalcs <- vector()
for(i in names(tempval)){
xx <- tempval[[i]]
xx$Date<-strptime(xx$Date,"%Y-%m-%d", tz="GMT")
year = as.numeric(format(xx$Date, "%Y"))
month = as.numeric(format(xx$Date, "%m"))
day = as.numeric(format(xx$Date, "%d"))
Tmin = data.frame(year, month, day, T = xx$Tmin)
Tmax = data.frame(year, month, day, T = xx$Tmax)
hrly = vector()
for(j in 1:nrow(xx)){
xy <- Th_interp(Tmin, Tmax,
day = j,
tab_calibr = calibration_l$Average)
hrly = rbind(hrly,
data.frame(
date = xx[j,'Date'],
Temp = xy$Th,
Year = Tmin$year[j],
JDay = as.numeric(format(xx[j,'Date'], "%j")),
month = Tmin$month[j],
day = Tmin$day[j],
Hour = 1:24
)
)
}
# Skip if NA for temperature data
if(apply(hrly, 2, function(x) all(!is.na(x)))["Temp"]) {
chillcalc <- chilling(hrly, hrly$JDay[1], hrly$JDay[nrow(hrly)]) #
} else { chillcalc <- data.frame("Season"=NA,"End_year"=NA,"Chilling_Hours"=NA, "Utah_Model"=NA, "Chill_portions"=NA) }
chillcalcs <- rbind(chillcalcs, data.frame(datasetIDlatlong = i,chillcalc[c("Season","End_year","Chilling_Hours","Utah_Model","Chill_portions")]))
#i think this will work: lat and long are part of dataset id column chillcalcs output table
}
head(chillcalc)
head(chillcalcs)
dim(chillcalcs)
dups <- chillcalcs[duplicated(chillcalcs$datasetIDlatlong),]#for some reason, there are 14 study-field sample combinations that have duplicate chilling estimates. not sure why....and not sure which estimate to use.
dups
ospree <- read.csv("analyses/output/ospree_clean_withchill.csv", header=TRUE)
ospree<-subset(ospree, woody=="yes")
#First, just explore a bit:
dim(ospree)
head(ospree)
sort(unique(ospree$respvar.simple))#the different response variables
#Look at just days to bb data:
ospree.daysbb <- ospree[which(ospree$respvar.simple=="daystobudburst"),]
ospree.daysbb$response<-as.numeric(ospree.daysbb$response)
ospree.daysbb$Total_Chilling_Hours<-as.numeric(ospree.daysbb$Total_Chilling_Hours)
ospree.daysbb$forcetemp<-as.numeric(ospree.daysbb$forcetemp)
quartz()
hist(ospree.daysbb$response)
plot(ospree.daysbb$Total_Chilling_Hours,ospree.daysbb$response.time)
plot(ospree.daysbb$Total_Chilling_Hours,ospree.daysbb$response, ylim=c(0,5))
ospree.percbb <- ospree[which(ospree$respvar=="percentbudburst"),]
quartz()
plot(ospree.percbb$Total_Chilling_Hours,ospree.percbb$response)
dim(ospree.percbb) # 3281  rows
ospree.percbb$spp <- paste(ospree.percbb$genus, ospree.percbb$species, sep=".")
ospree.percbb <- subset(ospree.percbb, is.na(spp)==FALSE)
ospree.percbb<-subset(ospree.percbb,as.numeric(ospree.percbb$response)<800)#remove the weird value for perc budburst
#response variable = percent budburst:
ospree.percbb$response <- as.numeric(ospree.percbb$response)#this is percbudburst
##random effects
ospree.percbb$datasetID <- as.factor(ospree.percbb$datasetID)
ospree.percbb$spp <- as.factor(ospree.percbb$spp)
#predictors:
ospree.percbb$responsedays <- as.numeric(ospree.percbb$response.time)
ospree.percbb$totalchill <- as.numeric(ospree.percbb$Total_Chilling_Hours)
ospree.percbb$forcetemp <- as.numeric(ospree.percbb$forcetemp)
ospree.percbb$photoperiod_day <- as.numeric(ospree.percbb$photoperiod_day)
###create centered predictors so that we can compare their slopes
ospree.percbb$responsedays_cent <- as.numeric(ospree.percbb$response.time)-
mean(as.numeric(ospree.percbb$response.time),na.rm=TRUE)
ospree.percbb$totalchill_cent <- as.numeric(ospree.percbb$Total_Chilling_Hours)-
mean(as.numeric(ospree.percbb$Total_Chilling_Hours),na.rm=TRUE)
ospree.percbb$forcetemp_cent <- as.numeric(ospree.percbb$forcetemp)-
mean(as.numeric(ospree.percbb$forcetemp),na.rm=TRUE)
ospree.percbb$photoper_cent <- as.numeric(ospree.percbb$photoperiod_day)-
mean(as.numeric(ospree.percbb$photoperiod_day),na.rm=TRUE)
###create zscore predictors
ospree.percbb$responsedays_z <- (as.numeric(ospree.percbb$response.time)-
mean(as.numeric(ospree.percbb$response.time),na.rm=TRUE))/sd(as.numeric(ospree.percbb$response.time),na.rm=TRUE)
plot(ospree.daysbb$Total_Chilling_Hours,ospree.daysbb$response.time)
quartz()
hist(ospree.daysbb$response)
plot(ospree.daysbb$Total_Chilling_Hours,ospree.daysbb$response.time)
plot(ospree.daysbb$Total_Chilling_Hours,ospree.daysbb$response, ylim=c(0,5))
plot(ospree.daysbb$response, ospree.daysbb$response.time,ylim=c(0,5))
plot(ospree.daysbb$response, ospree.daysbb$response.time))
plot(ospree.daysbb$response, ospree.daysbb$response.time)
dups <- chillcalcs[duplicated(chillcalcs$datasetIDlatlong),]#for some reason, there are 14 study-field sample combinations that have duplicate chilling estimates. not sure why....and not sure which estimate to use.
dups
chillcalcs
tempval[197]
tempval[1]
ospree <- read.csv("analyses/output/ospree_clean.csv", header=TRUE)
ospree.daysbb <- ospree[which(ospree$respvar.simple=="daystobudburst"),]
plot(ospree.daysbb$response,ospree.daysbb$response.time)
ospree.percbb[which(spree.daysbb$response>1),]
ospree.percbb[which(ospree.daysbb$response>1),]
ospree.percbb[which(ospree.daysbb$response>1),]$datasetID
ospree.daysbb[which(ospree.daysbb$response>1),]$datasetID
ospree.daysbb[which(ospree.daysbb$response>1),]$datasetID
unique(ospree.daysbb[which(ospree.daysbb$response>1),]$datasetID)
chillcalcs
tempval[1]
tempval[2]
head(chillcalcs)
write.csv(chillcalcs,"analyses/output/fieldchillcalcslatlong.csv",row.names=FALSE, eol="\r\n")
options(stringsAsFactors=FALSE)
setwd("~/git/ospree")
dat <- read.csv("analyses/output/ospree_clean.csv") #this file should use the cleaned data file created from Lizzie's "cleanmerge_all.R" code, after the chilltemps and chilldays have been cleaning with cleanin_chilltemp.R
dat <- read.csv("analyses/output/ospree_clean.csv") #this file should use the cleaned data file created from Lizzie's "cleanmerge_all.R" code, after the chilltemps and chilldays have been cleaning with cleanin_chilltemp.R
colnames(dat)[17]<-"fsdate_tofix"#the date format in this new file needs to be changed, for this code to work
dat$fieldsample.date<-strptime(strptime(dat$fsdate_tofix, format = "%m/%d/%Y"),format = "%Y-%m-%d")
#this file should use the cleaned data file created from Lizzie's "cleanmerge_all.R" code
colnames(dat)[17]<-"fsdate_tofix"#the date format in this new file needs to be changed, for this code to work
dat$fieldsample.date<-strptime(strptime(dat$fsdate_tofix, format = "%d-%b-%Y"),format = "%Y-%m-%d")
#use only woody species
dat2 <- subset(dat, woody=="yes")
# make two data frames. North America and Europe, the lat longs and years.
dat2$continent <- tolower(dat2$continent)
dat2$datasetID <- as.character(dat2$datasetID)
dat2$provenance.lat <- as.numeric(as.character(dat2$provenance.lat))
dat2$provenance.long <- as.numeric(as.character(dat2$provenance.long))
dat2$year <- as.numeric(as.character(dat2$year))
dat2$fieldsample.date<-as.character(as.Date(dat2$fieldsample.date,"%m/%d/%y")) #needed for new version
dat2 <- as_data_frame(dat2)
#Make a column that indexes the study, provenance latitude,provenance longitude, and field sample date, in order to calculate field chilling
dat2$ID_fieldsample.date<-paste(dat2$datasetID,dat2$provenance.lat,dat2$provenance.long,dat2$fieldsample.date, sep="_")
#Make a column that indexes the experimental chilling treatment (including chilltemp, chillphotoperiod & chilldays), in order to calculate field chilling
dat2$ID_chilltreat<-paste(dat2$datasetID,dat2$chilltemp,dat2$chilldays,sep=".")
##### Calculate experimental chilling, using chillday and chilltemp.
##there are many non-numeric values in the chilltemp and chilldays columns- these are unusable currently so remove:
#want table with datasetID chilling days, chilling temperature,  treat for each study.
chilldat <- dat2 %>% # start with the data frame
distinct(ID_chilltreat,.keep_all = TRUE) %>% # establishing grouping variables
dplyr::select(datasetID, chilltemp, chilldays, year,ID_chilltreat)
chilldat$chilltemp<-as.numeric(chilldat$chilltemp)
chilldat$chilldays<-as.numeric(chilldat$chilldays)
chilldat<- chilldat[apply(chilldat, 1, function(x) all(!is.na(x))),] # only keep rows of all not na
expchillcalcs <- vector()
###First, need file with hrly temperature data for each row in dataframe
for(i in 1:nrow(chilldat)) {
# Skip if NA for chilltemp or chilldays data
if(!is.na(chilldat$chilltemp[i]) & chilldat$chilldays[i] !=0 & !is.na(chilldat$chilldays[i])) {
yr <- as.numeric(chilldat$year[i])
if(is.na(yr)){ yr <- 2016} #temporary fix for when years are not currently listed!
hrly =
data.frame(
Temp = rep(as.numeric(chilldat$chilltemp[i]),
times = 24 * round(as.numeric(chilldat$chilldays[i], digits=0))),
Year = rep(yr,
times = 24 * round(as.numeric(chilldat$chilldays[i],digits=0))),
JDay = sort(rep(as.numeric(seq(1:round(as.numeric(chilldat$chilldays[i], digits=0)))), times = 24))
)
expchillcalc <- chilling(hrly, hrly$JDay[1], hrly$JDay[nrow(hrly)])
} else { expchillcalc <- data.frame("Chilling_Hours"=NA, "Utah_Model"=NA, "Chill_portions"=NA) }
expchillcalcs <- rbind(expchillcalcs,
data.frame(datasetID = chilldat$datasetID[i],
ID_chilltreat = chilldat$ID_chilltreat[i],
expchillcalc[c("Chilling_Hours","Utah_Model","Chill_portions")]))
}
colnames(expchillcalcs)[3:5] <- c("Exp_Chilling_Hours","Exp_Utah_Model","Exp_Chill_portions")
###Merge field and experimental chilling data with the rest of the data
# Add experimental chilling. Right number of rows = 12916 rows, 60 columns
dat3 <- merge(dat2, expchillcalcs,
by.x = c("datasetID","ID_chilltreat"),
by.y=c("datasetID","ID_chilltreat"),
all.x=T)
#Add field chilling calculations to datafile. #still 12916 rows, now 63 columns (3 ways of estimating experimental chilling)
###First, read in chillcalc file, so that you don't have to run the above code with the external hard drive of climate data
chillcalcs <- read.csv("analyses/output/fieldchillcalcslatlong.csv", header=T)
dim(chillcalcs)
chillcalcs <- chillcalcs[apply(chillcalcs, 1, function(x) all(!is.na(x))),] # only keep rows of all not NA. 354 rows now.
dim(chillcalcs)
dim(dat3)
dim(dat2)
colnames(chillcalcs)
colnames(chillcalcs) <- c("ID_fieldsample.date","Season","End_year","Field_Chilling_Hours","Field_Utah_Model","Field_Chill_portions")
dim(chillcalcs)
chilldups <- chillcalcs[duplicated(chillcalcs$ID_fieldsample.date),]
chilldups
(nochillcalcs <- unique(dat3$ID_fieldsample.date[!dat3$ID_fieldsample.date %in% chillcalcs$ID_fieldsample.date]))
chillcalcs <- chillcalcs[chillcalcs$ID_fieldsample.date %in% dat3$ID_fieldsample.date,]
dim(chillcalcs)
dim(dat3)
dat4<-join(dat3, chillcalcs,by="ID_fieldsample.date",match="all")
library(dplyr)
dat4<-join(dat3, chillcalcs,by="ID_fieldsample.date",match="all")
library(plyr)
dat4<-join(dat3, chillcalcs,by="ID_fieldsample.date",match="all")
dim(dat4)
dim(dat3)
dat4<-full_join(dat3, chillcalcs, by="ID_fieldsample.date", match="all") #Added by Cat
dim(dat4)
dat4$Total_Chilling_Hours <- dat4$Exp_Chilling_Hours+dat4$Field_Chilling_Hours
dat4$Total_Utah_Model <- dat4$Exp_Utah_Model+dat4$Field_Utah_Model
dat4$Total_Chill_portions <- dat4$Exp_Chill_portions+dat4$Field_Chill_portions
#For sites with no experimental chilling, just use field chilling:
dat4[which(is.na(dat4$Exp_Chilling_Hours)),]$Total_Chilling_Hours<-dat4[which(is.na(dat4$Exp_Chilling_Hours)),]$Field_Chilling_Hours
dat4[which(is.na(dat4$Exp_Utah_Model)),]$Total_Utah_Model<-dat4[which(is.na(dat4$Exp_Utah_Model)),]$Field_Utah_Model
dat4[which(is.na(dat4$Exp_Chill_portions)),]$Total_Chill_portions<-dat4[which(is.na(dat4$Exp_Chill_portions)),]$Field_Chill_portions
#For sites with no field chilling, should we use only experimental chilling? not sure if this is ok...
dat4[which(is.na(dat4$Field_Chilling_Hours)),]$Total_Chilling_Hours<-dat4[which(is.na(dat4$Field_Chilling_Hours)),]$Exp_Chilling_Hours
dat4[which(is.na(dat4$Field_Utah_Model)),]$Total_Utah_Model<-dat4[which(is.na(dat4$Field_Utah_Model)),]$Exp_Utah_Model
dat4[which(is.na(dat4$Field_Chill_portions)),]$Total_Chill_portions<-dat4[which(is.na(dat4$Field_Chill_portions)),]$Exp_Chill_portions
write.csv(dat4,"analyses/output/ospree_clean_withchill.csv",row.names=FALSE, eol="\r\n")
dat <- read.csv("analyses/output/ospree_clean.csv") #should use the cleaned data file created from Lizzie's "cleanmerge_all.R"" code, after the chilltemps and chilldays have been cleaned with cleaning_chilltemp.R" code
dat2 <- subset(dat, woody=="yes")
unique(dat2$chilltemp)#Need to fix
dat2[which(dat2$chilltemp=="Chilling treatment at 0.7 \xb1 0.7 C interrupted by mild spells of 14 days duration at a constant temperature of 8 or 12 C"),]#jones12
dat2[which(dat2$chilltemp=="<16"),]#jones12
unique(dat2[which(dat2$chilltemp=="<16"),]$datasetID)#jones12
unique(dat2$chilltemp)#Need to fix
