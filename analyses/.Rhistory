geom_point(aes(shape=phase, color=sector)) + ylab(my.xlab) + theme(axis.title.y=element_blank()) +
geom_hline(yintercept=0, linetype=2) +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
ylim(c(-20, 10))
temp<- temp + scale_x_discrete(labels=c("All species - soft freeze (Augspurger, 2013)"="All species - soft freeze (Augspurger, 2013)",
"All species (Peterson & Abatzoglou, 2014)"="All species (Peterson & Abatzoglou, 2014)",
"All species - hard freeze (Schwartz, 1993))"="All species - hard freeze (Schwartz, 1993))",
"Fagus sylvatica - 50% (Lenz et al., 2016)" = expression(paste(italic("Fagus sylvatica"),  "- 50% (Lenz et al., 2016)")),
"Eucalyptus pauciflora (Barker et al., 2005)"=expression(paste(italic("Eucalyptus pauciflora"), "(Barker et al., 2005)")),
"Acer pseudoplatanus - 50% (Lenz et al., 2016)"=expression(paste(italic("Acer pseudoplatanus"), "- 50% (Lenz et al., 2016)")),
"Tilia platyphyllos - 50% (Lenz et al., 2016)"=expression(paste(italic("Tilia platyphyllos"), "- 50% (Lenz et al., 2016)")),
"Sorbus aucuparia - 50% (Lenz et al., 2016"=expression(paste(italic("Sorbus aucuparia"), "- 50% (Lenz et al., 2016")),
"Prunus avium - 50% (Lenz et al., 2016)"=expression(paste(italic("Prunus avium"), "- 50% (Lenz et al., 2016)")),
"Rice - 100% (Sanchez et al., 2013)"="Rice - 100% (Sanchez et al., 2013)",
"All species (Cannell & Smith, 1986)"="All species (Cannell & Smith, 1986)",
"Corn - 100% (Sanchez et al., 2013)"="Corn - 100% (Sanchez et al., 2013)",
"Vaccinium spp. (Longstroth, 2012)"=expression(paste(italic("Vaccinium spp."), "(Longstroth, 2012)")),
"Wheat - 10 to 90% (Barlow et al., 2015)"="Wheat - 10 to 90% (Barlow et al., 2015)",
"Wheat - 100% (Barlow et al., 2015)"="Wheat - 100% (Barlow et al., 2015)",
"Rosaceae - 10% (Longstroth, 2013)"=expression(paste(italic("Rosaceae"), "- 10% (Longstroth, 2013)")),
"Rosaceae - 90% (Longstroth, 2013)"=expression(paste(italic("Rosaceae"), "- 90% (Longstroth, 2013)")),
"Wheat - 100% (Sanchez et al., 2013)"="Wheat - 100% (Sanchez et al., 2013)"))
temp +coord_flip()
Vcmax <- function (Vmo, Tvlo, Tv) {  #Vmo = 23, Tvlo = 277.15 are constants
(Vmo * (exp(3000 * (1/288.15 -1/Tv)))) /
((1 + exp(.4 * (Tvlo - Tv))) * (1 + exp(0.4 * (Tv - 318.15))))
}
## compensation point
comp <- function(Tv) { 21.2 * exp(5000 * ((1/288.15) - (1/Tv))) } #units: umol/mol
## K1 and K2
K1 <- function(Tv) { 150 * exp(6000 * ((1/288.15) - (1/Tv))) }
K2 <- function(Tv) {0.836 * exp(-1400 * ((1/288.15) - (1/Tv))) }
## el and ea
convertTv <- function(Tv) {
TvC <- Tv -273.15
return(TvC)
}
el <- function(Tv) {
TvC <- convertTv(Tv)
el <- 0.611 * exp((17.502 * TvC)/(TvC + 240.97))
return(el)
}
ea <- function (relHum, Tv) {
relHum /(100 * el(Tv))  # changed from multiplication to divide; Campbell and Norman page 42
}
input.df <- dummy
farquhar_solver <- function (input.df, stomata = c('open', 'closed')) {
## input.df needs to have columns for Tv, PAR, Ca and relhum. the rest can be calculated...
## defaults that we'd like to avoid including in the giant function call
gamma <- 0.015
Vmo <- 23 #92 #35
b <- 2
alpha <- 0.04
Do <- 1.3
M <- 9  #stomatal slope; unitless; Raczka et al 2016, Biogeosciences; (also Heroult 2013, Plant Cell & Environment)
Tvlo <- 277.85
## Functions used to calculate other inputs from input.df
input.df$Vcmax <- Vcmax(Vmo = Vmo, Tvlo = Tvlo, Tv = input.df$Tv )
input.df$comp <- comp(Tv = input.df$Tv)
input.df$el <- el(Tv = input.df$Tv)
input.df$ea <- ea(Tv = input.df$Tv, relHum = input.df$relHum)
# calculated inputs
input.df$Rd <- gamma * input.df$Vcmax
input.df$k1.dat <- K1(input.df$Tv)
input.df$k2.dat <- K2(input.df$Tv)
# substitutions for simplification in quadratics
input.df$X = input.df$k1.dat * (1 + input.df$k2.dat) # X = K1*(1+K2)
input.df$Y <- alpha * input.df$PAR  # Y = alpha*PAR
# F = (Ca - comp) * (1 + ((el - ea)/Do))
input.df$FF <- ((input.df$Ca - input.df$comp) * (1 + ((input.df$el - input.df$ea)/Do)))
### relevant functions
# A1 = [(Vcmax * (Ci - comp)) / (Ci + K1 * (1+K2)) ] - Rd
# A1 = [(Vcmax * (Ci - comp)) / (Ci + X) ] - Rd
A1<- function(Ci, input.df) {
A <- ((input.df$Vcmax * (Ci - input.df$comp)) / (Ci + input.df$X)) - input.df$Rd
return(A)
}
# A2 = [ alpha * PAR * (Ci - comp) / (Ci + 2comp) ] - Rd
A2 <- function(Ci, input.df) {
A <- (alpha * input.df$PAR * ((Ci - input.df$comp) / (Ci + 2 * input.df$comp))) - input.df$Rd
return(A)
}
#
# #Actually want to be solving for and minimizing J (post-quadratic) and minimizing, THEN subtracting R
#
# J1 <- function(Ci, input.df) {
#    J <- ((input.df$Vcmax * (Ci - input.df$comp)) / (Ci + input.df$X))
#    return(J)
# }
#
# J2 <- function(Ci, input.df) {
#    J <- (alpha * input.df$PAR * ((Ci - input.df$comp) / (Ci + 2 * input.df$comp)))
#    return(J)
# }
#
### stomata closed ###
if(stomata == 'closed') {
### CO2 limited case, coefficients for polyroot function
aa <- (b * input.df$X * input.df$Ca/1.6) + (input.df$Vcmax * input.df$comp) + (input.df$Rd * input.df$X)
bb <- (input.df$Rd) + (b * input.df$Ca / 1.6) - (b * input.df$X / 1.6) - (input.df$Vcmax)
cc <- (-b / 1.6)
z <- data.frame(aa = aa, bb = bb, cc = cc)  # where aa + bb*c1 + cc*c1^2
#solve the polynomial
roots <- apply(z, 1, polyroot)
if(round(Im(roots[1]), 10) != 0) {
stop("quadratic roots are imaginary")
}
#coerce into non-imaginary components
roots.num <- Re(roots)
#extract the non-negative value
Ci.extract.A1 <- apply(roots.num, 2, max)
# calculate A
AA1 <- A1(Ci = Ci.extract.A1, input.df = input.df)
### Light limited case, coefficients for polyroot function
aa <- ((b * 2 * input.df$comp * input.df$Ca / 1.6) + (input.df$Rd * 2 * input.df$comp) + (input.df$Y * input.df$comp))
bb <- (input.df$Rd + (b * input.df$Ca / 1.6) - (b * 2 * input.df$comp / 1.6) - input.df$Y)
cc <- (-b / 1.6)
# define polynomial roots for each data point
z <- data.frame(aa = aa, bb = bb, cc = cc)  # where aa + bb*c1 + cc*c1^2
#solve the polynomial
roots <- apply(z, 1, polyroot)
if(round(Im(roots[1]), 10) != 0) {
stop("quadratic roots are imaginary")
}
#coerce into non-imaginary components
roots.num <- Re(roots)
# extract the non-negative value
Ci.extract.A2 <- apply(roots.num, 2, max)
# calculate A2
AA2 <- A2(Ci = Ci.extract.A2, input.df = input.df)  # only works if PAR has values 6 orders of magnitude higher
### Build output data frame
# pick minimum for each time point:
A.df <- data.frame (AA1 = AA1, AA2 = AA2, Ci.A1 = Ci.extract.A1, Ci.A2 = Ci.extract.A2)
A.df$A.min <- apply(A.df[,1:2], 1, min)
A.df$min.eq <- apply(A.df[,1:2], 1, which.min)
## Solve for gsw ##
# stomata closed, gsw = b
A.df$gsw <- rep(b, dim(A.df)[1])
}
#### Stomata Open ###
if(stomata == 'open') {
############ vvvvv THIS IS WRONG!!!!!!! vvvvv ************
### CO2 limited coefficients
aa <- ((b * input.df$X * input.df$Ca * input.df$FF) + (1.6 * input.df$FF * input.df$Vcmax * input.df$comp) - (1.6 * input.df$FF * input.df$Rd * input.df$X) -
(M * input.df$Vcmax * input.df$comp * input.df$Ca) + (M * input.df$Rd * input.df$X * input.df$Ca))
bb <- ((-input.df$FF * b * input.df$Ca) - (b * input.df$X) - (1.6 * input.df$FF * input.df$Vcmax) + (1.6 * input.df$FF * input.df$Rd) + (M * input.df$Vcmax * input.df$Ca) +
(M * input.df$Vcmax * input.df$comp) - (input.df$Rd * M * input.df$Ca) + (M * input.df$Rd * input.df$X))
cc <- ((-b * input.df$FF) + (M * input.df$Vcmax) - (M * input.df$Rd))
############ ^^^^^ THIS IS WRONG!!!!!!! ^^^^^ ************
# define polynomial roots for each data point
z <- data.frame(aa = aa, bb = bb, cc = cc)  # where aa + bb*c1 + cc*c1^2
#solve the polynomial
roots <- apply(z, 1, polyroot)
if(round(Im(roots[1]), 10) != 0) {
stop("quadratic roots are imaginary")
}
#coerce into non-imaginary components
roots.num <- Re(roots)
#extract the non-negative value
Ci.extract.A1 <- apply(roots.num, 2, max)
#calculate A1
AA1 <- A1(Ci = Ci.extract.A1, input.df = input.df)
### Light-limited coefficients
aa <- ((b * input.df$FF * input.df$Ca * 2 * input.df$comp) - (1.6 * input.df$FF * input.df$Y * input.df$comp) + (1.6 * input.df$FF * input.df$Rd * 2 * input.df$comp) -
(M * input.df$Ca * input.df$Y * input.df$comp) + (M * input.df$Rd * 2 * input.df$comp * input.df$Ca))
bb <- ((b * input.df$FF * input.df$Ca) - (b * input.df$FF * 2 * input.df$comp) - (1.6 * input.df$FF * input.df$Y) + (1.6 * input.df$FF * input.df$Rd) + (M * input.df$Y * input.df$Ca) +
(M * input.df$Y * input.df$comp) - (M * input.df$Rd * input.df$Ca) - (M * input.df$Rd * 2 * input.df$comp))
cc <- ((-b * input.df$FF) - (M * input.df$Y) + (M * input.df$Rd))
# define polynomial roots for each data point
z <- data.frame(aa = aa, bb = bb, cc = cc)  # where aa + bb*c1 + cc*c1^2
#solve the polynomial
roots <- apply(z, 1, polyroot)
if(round(Im(roots[1]), 10) != 0) {
stop("quadratic roots are imaginary")
}
# coerce into non-imaginary components
roots.num <- Re(roots)
# extract the non-negative value
Ci.extract.A2 <- apply(roots.num, 2, max)
# calculate A2
AA2 <- A2(Ci = Ci.extract.A2, input.df = input.df)  # only works if PAR has values 6 orders of magnitude higher
### build output data frame
# pick minimum for each time point:
A.df <- data.frame (AA1 = AA1, AA2 = AA2, Ci.A1 = Ci.extract.A1, Ci.A2 = Ci.extract.A2)
A.df$A.min <- apply(A.df[,1:2], 1, min)
A.df$min.eq <- apply(A.df[,1:2], 1, which.min)
### solve for gsw ###
gsw.solve <- ((M * A.df$A.min)/((input.df$Ca - input.df$comp) * (1 + ((input.df$el - input.df$ea)/Do)))) + b
A.df$gsw <- gsw.solve
}
return(A.df)
}
##### Applying the farquhar_solver function #####
farquhar_solver(input.df = dummy, stomata = 'closed')
farquhar_solver(input.df = dummy, stomata = 'open')
#### Check against plantecophs package... ####
dummy$el <- el(Tv = dummy$Tv)
dummy$ea <- ea(relHum = dummy$relHum, Tv = dummy$Tv)
dummy$VPD<- dummy$el-dummy$ea
dummy$Tair<-dummy$Tv-273.15
FARAO(Ca=dummy$Ca, VPD=dummy$VPD, Tair=dummy$Tair)
#### Now look at real data to compare ####
dat <- read.csv('~/Documents/git/scaling_farquhar/Aggregated_Climate_Data.csv', header=TRUE)
dat$Tv <- dat$Air_Temp_K
dat$Tv<-ave(dat$Tv, dat$month, dat$year, dat$day, dat$hour)
dat$relHum <- dat$Relative_Humidity_Percent
dat$relHum<-ave(dat$relHum, dat$month, dat$year, dat$day, dat$hour)
dat$Ca <- as.numeric(dat$Atmospheric_CO2)
dat$Ca <- ave(dat$Ca, dat$month, dat$year, dat$day, dat$hour)
dat$comp <- comp(Tv = dat$Tv)
dat$comp<-ave(dat$comp, dat$month, dat$year, dat$day, dat$hour)
dat$Vcmax <- Vcmax(Vmo = 35, Tv = dat$Tv, Tvlo=277.85)
dat$Vcmax<-ave(dat$Vcmax, dat$month, dat$year, dat$day, dat$hour)
dat$el <- el(Tv = dat$Tv)
dat$el<-ave(dat$el, dat$month, dat$year, dat$day, dat$hour)
dat$ea <- ea(relHum = dat$relHum, Tv = dat$Tv)
dat$ea<-ave(dat$ea, dat$month, dat$year, dat$day, dat$hour)
dat$PAR <- dat$Par_moles_m2_s * 1000000 ### only keep this until push updated to git
dat$PAR<-ave(dat$PAR, dat$month, dat$year, dat$day, dat$hour)
dat$time<-dat$hour
library(dplyr)
dat.check<-dat%>%filter(year==2013)%>%filter(month==8)%>%filter(day==13)%>% ### choose this date due to LAI information
dplyr::select(hour, Tv, relHum, Ca, comp, Vcmax, el, ea, PAR, time)
dat.check<-dat.check[!duplicated(dat.check),]
dat.check<-dat.check[(dat.check$time<24),]
library(plantecophys)
dat.check$VPD<- dat.check$el-dat.check$ea
#dat.check$VPDx<-RHtoVPD(dat.check$relHum, TdegC = dat.check$Tv-273.15)
dat.check$Tair<-dat.check$Tv-273.15
FARAO(Ca=dat.check$Ca, VPD=dat.check$VPD, Tair=dat.check$Tair, PPFD=dat.check$PAR)
lai <- read.csv('~/Documents/git/scaling_farquhar/hf150-01-hem-lai.csv', header=TRUE)
lai<-lai%>%filter(date=="2013-08-13")
lai$lai.sum<-ave(lai$lai.masked, FUN=sum) ## 54.28
###make a data frame for FARAO out put
eco.phys.sol<-as.data.frame(FARAO(Ca=dat.check$Ca, VPD=dat.check$VPD, Tair=dat.check$Tair))
##add a column for the lai sums from harvard forest
eco.phys.sol$HF.lai<-lai$lai.sum
###multiply them together to get the footprint level A
eco.phys.sol$A.footprint<-eco.phys.sol$HF.la*eco.phys.sol$ALEAF
eco.phys.sol$hour<-1:24
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))
library(ggplot2)
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))
colnames(eco.phys.sol)
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point()
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point() + geom_line(aes(x=hour, y=ALEAF))
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point(col="green") + geom_line(aes(x=hour, y=ALEAF), col="green") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.ticks.y = element_blank())
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point(col="forestgreen") + geom_line(aes(x=hour, y=ALEAF), col="forestgreen") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.ticks.y = element_blank())
farquhar_solver(input.df = dat.check, stomata = 'open')
far<-as.data.frame(farquhar_solver(input.df = dat.check, stomata = 'open'))
eco.phys.sol$A<-far$AA2
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point(col="forestgreen") + geom_line(aes(x=hour, y=ALEAF), col="forestgreen") +
geom_line(aes(x=hour, y=A), col="steelblue") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.ticks.y = element_blank())
eco.phys.sol$A<-far$AA1
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point(col="forestgreen") + geom_line(aes(x=hour, y=ALEAF), col="forestgreen") +
geom_line(aes(x=hour, y=A), col="steelblue") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.ticks.y = element_blank())
eco.phys.sol$A<-far$A.min
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point(col="forestgreen") + geom_line(aes(x=hour, y=ALEAF), col="forestgreen") +
geom_line(aes(x=hour, y=A), col="steelblue") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.ticks.y = element_blank())
#### get the data
bb.stan<-read.csv("~/Documents/git/regionalrisk/analyses/output/bb.brm.nointer.csv", header=TRUE)
## housekeeping
rm(list=ls())
options(stringsAsFactors = FALSE)
## Libraries
library(brms)
head(bb.stan)
#### get the data
bb.stan<-read.csv("~/Documents/git/regionalrisk/analyses/output/bb.brm.nointer.csv", header=TRUE)
head(bb.stan)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(nao==-1.583333)
library(dplyr)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(nao==-1.583333)
#### get the data
bb.stan<-read.csv("~/Documents/git/regionalrisk/analyses/output/bb.brm.nointer.csv", header=TRUE)
bb.stan<-bb.stan%>%filter(species=="AESHIP")
head(bb.stan)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(space>-5)%>%filter(space<10)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(space>-5)%>%filter(space<5)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(space>-3)%>%filter(space<3)
brm<-brm(fs.count~nao, data=bb.stan, chains=2, family=poisson)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(space>-1)%>%filter(space<2)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(space>0)%>%filter(space<1)
brm<-brm(fs.count~nao, data=bb.stan, chains=2, family=poisson)
save(brm, file="~/Desktop/brm.Rdata")
load("/Users/CatherineChamberlain/Desktop/brm.Rdata")
brm
d<-read.csv("~/Desktop/sandbox/buds_traits.csv")
d<-read.csv("~/Desktop/sandbox/buds_obs.csv")
View(d)
d<-read.csv("~/Documents/git/freezingexperiment/analyses/output/buds_traits.csv")
View(d)
rm(list=ls())
options(stringsAsFactors = FALSE)
fit<-brm(dvr~frost+bud+(1|species)+(frost-1|species)+(bud-1|species), data=d)
d<-read.csv("/n/wolkovich_lab/Lab/Cat/sandbox/buds_traits.csv")
d<-read.csv("~/Documents/git/freezingexperiment/analyses/output/buds_traits.csv")
fit<-brm(dvr~frost+bud+(1|species)+(frost-1|species)+(bud-1|species), data=d)
m<-fit
m.int<-posterior_interval(m)
sum.m<-summary(m)
cri.f<-as.data.frame(sum.m$fixed[,c("Estimate", "l-95% CI", "u-95% CI")])
cri.f<-cri.f[-1,] #removing the intercept
fdf1<-as.data.frame(rbind(as.vector(cri.f[,1]), as.vector(cri.f[,2]), as.vector(cri.f[,3])))
fdf2<-cbind(fdf1, c(0, 0, 0) , c("Estimate", "2.5%", "95%"))
names(fdf2)<-c(rownames(cri.f), "species", "perc")
cri.r<-(ranef(m, summary = TRUE, robust = FALSE,
probs = c(0.025, 0.975)))$species
cri.r2<-cri.r[, ,-1]
cri.r2<-cri.r2[,-2,]
dims<-dim(cri.r2)
twoDimMat <- matrix(cri.r2, prod(dims[1:2]), dims[3])
mat2<-cbind(twoDimMat, c(rep(1:2, length.out=6)), rep(c("Estimate", "2.5%", "95%"), each=2))
df<-as.data.frame(mat2)
names(df)<-c(rownames(cri.f), "species", "perc")
dftot<-rbind(fdf2, df)
dflong<- tidyr::gather(dftot, var, value, nao:sm.elev, factor_key=TRUE)
dflong<- tidyr::gather(dftot, var, value, frost:bud, factor_key=TRUE)
#adding the coef estiamtes to the random effect values
for (i in seq(from=1,to=nrow(dflong), by=9)) {
for (j in seq(from=3, to=8, by=1)) {
dflong$value[i+j]<- as.numeric(dflong$value[i+j]) + as.numeric(dflong$value[i])
}
}
dflong$rndm<-ifelse(dftot$species>0, 2, 1)
dfwide<-tidyr::spread(dflong, perc, value)
dfwide[,4:6] <- as.data.frame(lapply(c(dfwide[,4:6]), as.numeric ))
dfwide$species<-as.factor(dfwide$species)
fit<-lm(dvr~frost+bud+(1|species)+(frost-1|species)+(bud-1|species), data=d)
fit<-lmer(dvr~frost+bud+(1|species)+(frost-1|species)+(bud-1|species), data=d)
load("/Users/CatherineChamberlain/Desktop/sandbox/fit.Rdata")
bb.stan<-read.csv("~/Documents/git/regionalrisk/analyses/output/bb.brm.inter.csv", header=TRUE)
bb.stan<-read.csv("~/Documents/git/regionalrisk/analyses/output/bb.brm.nointer.csv", header=TRUE)
bb.stan<-subset(bb.stan, select=c("fs.count, m.index, sp.temp, cc"))
bb.stan<-subset(bb.stan, select=c("fs.count", "m.index", "sp.temp", "cc"))
bb.stan<-bb.stan[!duplicated(bb.stan),]
load("/Users/CatherineChamberlain/Documents/git/regionalrisk/analyses/output/brm.Rdata")
brm.full.nointer
load("/Users/CatherineChamberlain/Documents/git/regionalrisk/brm.Rdata")
brm.full.nointer
bb.stan<-read.csv("~/Documents/git/regionalrisk/analyses/output/bb.brm.nointer.csv", header=TRUE)
bb.stan<-subset(bb.stan, select=c("fs.count", "m.index", "sp.temp", "cc", "sm.elev", "space"))
bb.stan<-bb.stan[!duplicated(bb.stan),]
bb.stan<-na.omit(bb.stan)
View(bb.stan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
## Libraries
library(brms)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
bb<-bb.stan[sample(nrow(bb.stan), 500), ]
brm.simple<-brm(fs.count~m.index+sp.temp+cc+sm.elev+space+m.index:cc+sp.temp:cc+
sm.elev:cc+space:cc, data=bb, chains=2, cores=4)
brm.simple
d<-read.csv("~/Documents/git/regionalrisk/analyses/output/bb.brm.nointer.csv")
colnames(d)
aes<-subset(d, species=="AESHIP")
length(unque(aes$space))
length(unique(aes$space))
d<-read.csv("~/Documents/git/regionalrisk/analyses/output/regrisk.cleaned.csv")
d<-d[!duplicated(d),]
d<-na.omit(d)
colnames(d)
aes<-subset(d, species=="AESHIP")
length(unique(aes$lat.long))
length(unique(aes$year))
bet<-subset(d, species=="BETPEN")
length(unique(aes$lat.long))
length(unique(bet$year))
aln<-subset(d, species=="ALNGLU")
length(unique(aln$lat.long))
length(unique(bet$lat.long))
fsy<-subset(d, species=="FAGSYL")
length(unique(fsy$lat.long))
fra<-subset(d, species=="FRAEXC")
length(unique(fra$lat.long))
length(unique(fra$year))
que<-subset(d, species=="QUEROB")
length(unique(que$lat.long))
length(unique(que$year))
rm(list=ls())
options(stringsAsFactors = FALSE)
# dostan = TRUE
library(rstan)
library(ggplot2)
library(shinystan)
#library(bayesplot)
library(rstanarm)
library(brms)
library(ggstance)
#library(forcats)
# Setting working directory. Add in your own path in an if statement for your file structure
if(length(grep("danflynn", getwd())>0)) {
setwd("~/Documents/git/ospree")
} else setwd("~/Documents/git/projects/treegarden/budreview/ospree/analyses")
if(length(grep("Ignacio", getwd()))>0) {
setwd("~/GitHub/ospree/analyses")
} else setwd("~/Documents/git/projects/treegarden/budreview/ospree/analyses")
setwd("~/Documents/git/ospree/analyses")
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
########################
#### get the data
source("lat_analysis/source/bbdataplease.R")
## (2) Deal with species
dim(bb.noNA)
d <- bb.noNA
source("lat_analysis/source/speciescomplex.R")
bb.noNA.wtaxa <- d
dim(bb.noNA.wtaxa)
unique(bb.noNA.wtaxa$complex)
# merge in labgroup (we could do this elsewhere someday)
bb.wlab <- merge(bb.resp, taxon, by=c("genus","species"), all.x=TRUE)
bb.wlab <- within(bb.wlab, { prov.lat <- ave(provenance.lat, complex, FUN=function(x) length(unique(x)))}) # multiple provenance.lats
bb.wlab <- subset(bb.wlab, bb.wlab$prov.lat>1)
bb.wlab.photo<- within(bb.wlab, { photo <- ave(photoperiod_day, complex, FUN=function(x) length(unique(x)))}) # multiple photoperiods
bb.wlab.photo <- subset(bb.wlab.photo, bb.wlab.photo$photo>1)
tt <- table(bb.wlab.photo$complex)### testing
bb.wlab<-bb.wlab.photo
# [1] "Betula_complex"        "Betula_pendula"        "Betula_pubescens"      "Fagus_sylvatica"
# [5] "Malus_domestica"       "Picea_abies"           "Picea_glauca"          "Pseudotsuga_menziesii"
# [9] "Ribes_nigrum"          "Ulmus_complex"
myspp<-c("Betula_pendula", "Betula_pubescens", "Fagus_sylvatica", "Picea_abies", "Pseudotsuga_menziesii", "Ulmus_complex")
bb.wlab<-dplyr::filter(bb.wlab, complex%in%myspp)
studies<-dplyr::select(bb.wlab, datasetID, complex)
studies<-studies[!duplicated(studies),]
studies<-within(studies, { studies <- ave(datasetID, complex, FUN=function(x) length(unique(x)))})
studies<-dplyr::select(studies, -datasetID)
studies<-studies[!duplicated(studies),]
columnstokeep <- c("datasetID", "genus", "species", "varetc", "woody", "forcetemp",
"photoperiod_day", "response", "response.time", "Total_Utah_Model",
"complex", "provenance.lat")
bb.wlab.sm <- subset(bb.wlab, select=columnstokeep)
## make a bunch of things numeric (eek!)
bb.wlab.sm$force <- as.numeric(bb.wlab.sm$forcetemp)
bb.wlab.sm$photo <- as.numeric(bb.wlab.sm$photoperiod_day)
bb.wlab.sm$chill <- as.numeric(bb.wlab.sm$Total_Utah_Model)
bb.wlab.sm$resp <- as.numeric(bb.wlab.sm$response.time)
bb.wlab.sm$lat<-as.numeric(bb.wlab.sm$provenance.lat)
## subsetting data, preparing genus variable, removing NAs
ospr.prepdata <- subset(bb.wlab.sm, select=c("resp", "chill", "photo", "force", "complex", "lat", "datasetID"))
dim(subset(bb.wlab.sm, is.na(chill)==FALSE & is.na(photo)==FALSE & is.na(force)==FALSE
& is.na(lat)==FALSE & is.na(datasetID)==FALSE))
ospr.stan <- ospr.prepdata[complete.cases(ospr.prepdata),]
ospr.stan$sp <- as.numeric(as.factor(ospr.stan$complex))
## Center? or Z-score?
ospr.stan$sm.chill<-ospr.stan$chill/240
## center the predictors:
#ospr.stan$force.cen <- ospr.stan$force-mean(ospr.stan$force,na.rm=TRUE)
#ospr.stan$photo.cen <- ospr.stan$photo-mean(ospr.stan$photo,na.rm=TRUE)
#ospr.stan$chill.cen <- ospr.stan$chill-mean(ospr.stan$chill,na.rm=TRUE)
#ospr.stan$lat.cen <- ospr.stan$lat-mean(ospr.stan$lat,na.rm=TRUE)
## z-score the predictors:
#ospr.stan$force.z <- (ospr.stan$force-mean(ospr.stan$force,na.rm=TRUE))/sd(ospr.stan$force,na.rm=TRUE)
#ospr.stan$photo.z <- (ospr.stan$photo-mean(ospr.stan$photo,na.rm=TRUE))/sd(ospr.stan$photo,na.rm=TRUE)
#ospr.stan$chill.z <- (ospr.stan$chill-mean(ospr.stan$chill,na.rm=TRUE))/sd(ospr.stan$chill,na.rm=TRUE)
#ospr.stan$lat.z <- (ospr.stan$lat-mean(ospr.stan$lat,na.rm=TRUE))/sd(ospr.stan$lat,na.rm=TRUE)
ospr.stan <- subset(ospr.stan, resp<600)
cp<-ggplot(ospr.stan, aes(x=chill, y=photo)) + geom_point(aes(col=as.factor(complex))) +
facet_wrap(~complex) + theme(legend.position = "none")
fp<-ggplot(ospr.stan, aes(x=force, y=photo)) + geom_point(aes(col=as.factor(complex))) +
facet_wrap(~complex) + theme(legend.position = "none")
cf<-ggplot(ospr.stan, aes(x=chill, y=force)) + geom_point(aes(col=as.factor(complex))) +
facet_wrap(~complex) + theme(legend.position = "none")
lf<-ggplot(ospr.stan, aes(x=lat, y=force)) + geom_point(aes(col=as.factor(complex))) +
facet_wrap(~complex) + theme(legend.position = "none")
pl<-ggplot(ospr.stan, aes(x=lat, y=photo)) + geom_point(aes(col=as.factor(complex))) +
facet_wrap(~complex) + theme(legend.position = "none")
cl<-ggplot(ospr.stan, aes(x=lat, y=chill)) + geom_point(aes(col=as.factor(complex))) +
facet_wrap(~complex) + theme(legend.position = "none")
cp
quartz()
cp
cp
fp
quartz()
fp
quartz()
cf
quartz()
lf
quartz()
pl
quartz()
cl
tt
length(unique(ospr.stan[ospr.stan$complex=="Ulmus_complex"]))
length(unique(ospr.stan[ospr.stan$complex=="Ulmus_complex"],))
length(unique(ospr.stan[ospr.stan$complex=="Ulmus_complex"]),)
length(unique(ospr.stan[(ospr.stan$complex=="Ulmus_complex"),]))
length(unique(ospr.stan[(ospr.stan$complex=="Betula_pendula"),]))
length(ospr.stan[(ospr.stan$complex=="Betula_pendula"),])
View(ospr.stan)
